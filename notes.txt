augmenting artifact images to completely balance out classes worsens performance.
same goes for per_image_standardization

recall is a better indicator of proportion of correctly identified positives than pure number of false negative.
instead of recall, f beta score can be used to adjust f1score in favor of recall so it also takes into account precision as it also has to be relatively high, not just recall. 
when class balancing is applied, model learns much faster.
lenet model reaches peak performance in 2 epochs, then training and validation diverge. maybe with layer parameter optimization it can improve.
kerastuner's prebuilt hypermodels wont be used for now as they are computationally expensive to tune which isn't worth when goal here is binary classification hile those models are more suitable for complex classification tasks.

hyperband finds sligtly better models than bayesian optimization but it takes it 2 times more time than for bayesop.

commented line 88 in .venv/lib/python3.10/site-packages/tensorboard/plugins/scalar/summary_v2.py as it prevented tensorboard from working with keras tuner.

Model notes
Fiachra's model had 3441 (13.44 KB) parameters.
CNN_20240630_1 has 384k (1.47 MB) parameters.

These are the best 3 models:
CNN_20240505_1 has Fbeta score 0.986. 5391 (21.06 KB) parameters.
CNN_20240505_2 has Fbeta score 0.992. 24115 (94.20 KB) parameters.
CNN_20240618_3 has Fbeta score 0.991. 1966 (7.68 KB) parameters.

Model CNN_20240618_3 is deemed the best as it least overfits on the training data, has small number of parameters and second highest Fbeta score.

Fbeta score is currently used with beta=2. It is the approximation that recall is twice as important as precision.
for current hypermodel configuration, hyperband peaks at around 0.993 fbeta score even when given additional ammount of resources. additional changes to the configuration will be required in order to increase performance.

Model CNN_20240618_3
False negatives (6 out of 1702 positives) are mostly thick and oversaturated meteors and ones with very thin trails.
False positives (50 out of 366 negatives) are mostly twisted lines and bright noisy artifacts. There is a chance of small portion of them being mislabeled.

TODO
experiment with padding when creating images for dataset.
